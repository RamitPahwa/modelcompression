\documentclass[../main]{subfiles}
\begin{document}
\begin{abstract}
%The abstract paragraph should be indented 1/2~inch (3~picas) on both left and right-hand margins.
%Use 10~point type, with a vertical spacing of 11~points.
%The word \textsc{Abstract} must be centered, in small caps, and in point size 12.
%Two line spaces precede the abstract. The abstract must be limited to one paragraph.

Deploying neural network models to mobile devices poses a unique challenge because these models have to remain lightweight while still being accurate.
The current state of the art model for image feature extraction are deep and wide.
Although significant amount of time is spent in designing a optimal neural architecture which stand outs in all three dimension that is accuracy, size and speed it largely remain sub-optimal for the task.
Unlike previous work we propose incorporating inference latency in a structured way of reducing a known Teacher network to Student network using Reinforcement learning \citet{ashok2017n2n} and training it using  Knowledge Distillation \citet{hinton2015distilling}.
Our proposed reward function allows us to search for architectures that have high accuracy, comparable to that of the state of the art deep Convolutional Neural Network (CNN) as well as low inference latency and reasonable sizes fit for deployment.
Our proposed system outperform the previous work \citet{ashok2017n2n} that is able to search more accurate models with 4.78x improvement in inference latency, with 3.53x smaller in size than Resnet on CIFAR10 dataset.
We further show the effectiveness of transfer learning by learning an architecture on one dataset to further reduce the time to obtain the architecture given a teacher architecture to about one fifth of the time to search an optimal architecture from scratch.
We generalize the above results on CIFAR100 and Caltech256 Dataset, with architecture families VGG \citet{simonyan2014very} and Resnet \citet{he2015deep}.





\end{abstract}
\end{document}
